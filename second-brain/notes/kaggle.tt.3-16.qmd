---
id: pxgymi3e60nyrx4km7mtus8
title: "Tabular Tuesday 3.16"
desc: "Regression with a Crab Age Dataset"
updated: 1685827641805
created: 1685827641805
format:
  html:
    code-fold: true
---

# Models' scores

| Models    |     Scores     |
| :------------ | :-------------: |
| Baseline          | `1.48572` |
| Random Forest       |  |
| XGBoost |  |
| SVM |  |

# To-do

- [ ] Crab Age Prediction (original dataset) : https://www.kaggle.com/datasets/sidhus/crab-age-prediction
- [ ] Make Synthetic Crab Age Data : https://www.kaggle.com/code/inversion/make-synthetic-crab-age-data/notebook
- [ ] features engineering !!
- [ ] data cleaning : normalization

```{r}
# Loading library()
library(tidyverse)
library(tidymodels)

# Helper packages
library(dotwhisker)  # for visualizing regression results
```

# Data Manipulation

## Import data

```{r}
train <- read_csv("./datasets/regression-crab-age/train.csv", col_names = TRUE)
test <- read_csv("./datasets/regression-crab-age/test.csv", col_names = TRUE)
```

## Data cleaning

### Type casting

The `Sex` variable should be a `factor` mode and not a `character`. I use the `as.factor()` function in both the training set and testing set.

```{r}
train$Sex <- as.factor(train$Sex)
test$Sex <- as.factor(test$Sex)
```

### 1.2.2 Scaling

# Exploratory data analysis

```{r}
glimpse(train)
```

```{r}
glimpse(test)
```
## 2.1 few `ggplot`

```{r}
train %>%
  ggplot(aes(x = `Viscera Weight`, y = Length, color = Sex)) +
  geom_point()
```

# Modeling

## Baseline model

I fit a linear regression model, using the `glm` engine, on all the data in `train.csv` to modelize the `Age` variable.

```{r}
lm_model <- linear_reg() %>%
  set_engine("glm") %>%
  fit(Age ~ ., data = train)

lm_model
```

```{r}
tidy(lm_model) %>% 
  dwplot(dot_args = list(size = 2, color = "black"),
    whisker_args = list(color = "black"),
    vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))
```

```{r}
lm_predict <- predict(lm_model, test)
```

## Tidymodels

### Initial dataset split

I start by splitting the *training dataset*, `train.csv` into a *train set* and 
a *test set* to evaluate which model performl best, without submitting any to 
kaggle.

```{r}
set.seed(789)
training_split <- initial_split(train, strata = "Age")
tidymodels_train <- training(training_split)
tidymodels_test <- testing(training_split)

training_split
```

### 3.2.1 Cross validation

Using the `vfold_cv()` function, I separate the training dataset, `tidymodels_train`, 
into 10 folds for cross validation purposes.

```{r}
# 10-fold cross-validation
folds_train <- vfold_cv(tidymodels_train, v = 10, repeats = 1)  

folds_train$splits[[5]]
```

### 3.2.x Random Forest

```{r}
rf_model <- rand_forest() %>%
  set_mode("regression") %>%
  fit(Age ~ ., data = tidymodels_train)

rf_model
```

```{r}
rf_predict <- predict(rf_model, tidymodels_test)
```

## Models evaluation

```{r}
collect_metrics(rf_predict)
```
# Submission

Creating a `submission.csv` file

```{r}
submission <- test %>%
              bind_cols(lm_predict) %>%
              select(id, .pred) %>%
              rename(Age = .pred) %>%
              write_csv("submission.csv")

submission
```