---
title: How Generative AI works
---

Generative AI took over in 2022 with GPT3 released.

## Pretraining

Generative AI requires huge amount of data to process in order to gain knoxwledge. In case of text <inference> it needs ton high quality and diverse documents. Most often these documents comes from the Internet. [üç∑ FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) is a good example of dataset curated by generative Ai startups like Open AI, Anthropic, Mistral, etc...

Even thought, it's *only" 44TB worth of document

### Dataset foundation

[Common Crawl](https://commoncrawl.org/) data are free and open source repository of web pages (over 250 billion in the last 18 years)

Steps:

![](fineweb-recipe.png)

1. **URL Filtering**: website's contents that lower the quality and/or diversity of the dataset is removed by filtering out their URL at the very begging. Therefore crawlers do not even bother to visit these webpage and do not collect data on it. This kind of website are categorize in adult, malware, cryptojacking (i.e hijack your machine to mine crypto on the backend), etc...
2. **Text Extraction**: crawler collects raw pages, in markup languages: HTML, CSS, javascript.

```html
<p>Recently, we released <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"><strong>üç∑ FineWeb</strong></a>, a new, large-scale
        (<strong>15-trillion tokens, 44TB disk space</strong>) dataset for LLM pretraining. FineWeb is derived from 96 <a href="https://commoncrawl.org/">CommonCrawl</a> snapshots and produces <strong>better-performing LLMs than other open pretraining datasets</strong>. To bring more clarity in machine learning and advance the open understanding of how to train good quality large language models, we carefully documented and ablated all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. The present long form report is a deep dive in how to create a large and high-quality web-scale dataset for LLM pretraining. The dataset itself, üç∑ FineWeb, is available <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">here</a>.
        </p>
```

Therefore we need to extract only text

3. **Language Filtering**:
4. **Gopher Filtering**:
5. **PII removal**: identifies and delete personal information
6. **Custom Filters**:
7. **C4 Filters**:
8. ** MinHash dedup**:

## Tokenization

Neulral Network requires a one dimensional sequence of symbols with a finite numbers of possible symbols. Therefore we need to transform our latin alphabet that we use, as human, to communicate into symbols that computer can understand and work with.

## Ressources

1. [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)
