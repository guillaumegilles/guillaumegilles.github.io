---
title: AI Milestones
draft: true
---

> Artificial intelligence, or AI, is technology that enables computers and machines to simulate human intelligence and problem-solving capabilities.

Artificial intelligence, AI, is technology that enables computers and machines to simulate human intelligence and problem-solving capabilities. AI is a field of computer Science and encompasses machine-learning and deep-learning. It has gone through many cycles. Artificial General Intelligence is a long way from here we stand, despite some sparks of Artificial General Intelligence: Early experiments with GPT-4.
Innovation does not appear from the blue. We are staying on the shoulder of giants. One day these giants will be Yan Le Cun, Yoshua Bnegio, Geooffrey Hinton, for future deep learning engineer yet to become. Like Alan Turing is our today giant…

The excitement surrounding foundation models like GPT and Llama has led to a flood of research papers—some insightful, some less so, and a few worthy of a place in the AI hall of fame. Understanding the history behind these developments is crucial, as it shows us how we got to where we are today. By grasping the fundamental building blocks of AI, you’ll find it much easier to navigate the latest trends. [reisHistoryMatters2024]

Let’s embark on a journey through the fundamental milestones of artificial intelligence. In this essay, let’s deep dive into paper that built the giant.

## 50's

1950: Alan Turing publishes Computing Machinery and Intelligence. Alan turing = "computingmachinery and intelligence mind - octobre 1950
"instead of trying to produce a program to simulate the adult mind, why not try to produce one which simulates the child's. If this were the subject to an appropriate course of education one would obtained the adult brain."
In this paper, Turing—famous for breaking the German ENIGMA code during WWII and often referred to as the “father of computer science”— asks the following question: “Can machines think?” From there, he offers a test, now famously known as the “Turing Test,” where a human interrogator would try to distinguish between a computer and human text response. To pass the test, a computer must be able to fool a human into believing it is also human. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, as well as an ongoing concept within philosophy as it utilizes ideas around linguistics.
1956- Darmouth College
[John McCarthy](https://fr.wikipedia.org/wiki/John_McCarthy) coined the term artificial intelligence
= LISP est largement utilisé pour développer les algo d'AI à ce point : POURQUOI ?
1957: [Frank Rosenblatt](https://fr.wikipedia.org/wiki/Frank_Rosenblatt) - perceptron
1952 — Arthur Samuel wrote the first computer learning program. The program was the game of checkers, and the IBM computer improved at the game the more it played, studying which moves made up winning strategies and incorporating those moves into its program.
1956: John McCarthy coins the term “artificial intelligence” at the first-ever AI conference at Dartmouth College. (McCarthy would go on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program.

## 60’s: Perceptron and first neural network

1969: first Ai wintr [Perceptrons: An Introduction to Computational Geometry](https://direct.mit.edu/books/monograph/3132/PerceptronsAn-Introduction-to-Computational) : ils mopntrent les limtes des machines, puisque le papier vient du MIT et de la notoriété
1967 (or 1957??): Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that “learned” though trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research projects.
1967 — The “nearest neighbor” algorithm was written, allowing computers to begin using very basic pattern recognition. This could be used to map a route for traveling salesmen, starting at a random city but ensuring they visit all cities during a short tour.

## 70’s and 80’s: Régressions pénalisées et arbres

In the 70’ and 80’, artificial intelligence uses symbolic and logic reasoning, which has been called expert system and were called Good Old Fashion Artificial Intelligence, GOFAI, by the research community. [@LecunnLamachineapprend2023]
GOFAI : good old fashion artificial intelligence [@quandlamachineapprend], 
symbolic + logic = expert system
1979 — Students at Stanford University invent the “Stanford Cart” which can navigate obstacles in a room on its own.
1980s: Neural networks which use a backpropagation algorithm to train itself become widely used in AI applications.
1981 — Gerald Dejong introduces the concept of Explanation Based Learning (EBL), in which a computer analyses training data and creates a general rule it can follow by discarding unimportant data.
1985 — Terry Sejnowski invents NetTalk, which learns to pronounce words the same way a baby does.
1987 : NETtalk : http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
1987 back propagation paper : [Learning Internal Representations by Error Propagation](https://ieeexplore.ieee.org/document/6302929)
fin 88: LeNet (baptisé par Larry Jackell, en référence à Le Cunn) + DjVu 1988

## 90’s: Support Vector Machine et Random Forests

1997: deep blue - 40 years after darmouth college
1990s — Work on machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions — or “learn” — from the results.
1995: Stuart Russell and Peter Norvig publish Artificial Intelligence: A Modern Approach, which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems on the basis of rationality and thinking vs. acting.
1997: IBM’s Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).
1995: 2nd AI winter, researcher prefers SVM and kernel algo (https://en.wikipedia.org/wiki/Kernel_method) inventé En 92 et 96 par Isabelle Guyou, Vapnik Boser. Elles deviennent les techniques pahres entre 95 et 2010
1998: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
1998: LeNet-5 (training data: MNIST, 60k training examples - 10 classes) / CPU : Pentium II 0,27 GFLOPS / Algorithm : 60k parameters, 5 layers, Sigmoid Activation Function
1999: Boosting - ROb Schapire : http://rob.schapire.net/papers/Schapire99c.pdf + https://direct.mit.edu/books/oa-monograph/5342/BoostingFoundations-and-Algorithms

## 00’s: Méthodes d’agrégation, Boosting

2004: John McCarthy writes a paper, What Is Artificial Intelligence?, and proposes an often-cited definition of AI.
2006 — Geoffrey Hinton coins the term “deep learning” to explain new algorithms that let computers “see” and distinguish objects and text in images and videos.
2007 : atelier pirate à NIPS (https://neurips.cc/Conferences/2007) marque l'adoption du terme "deep learning" dans la littérature spécialisée. En 2007, les organisateurs de la principale conférence d'IA, NIPS (Neural Information Processing Systems), rejettent leur proposition de consacrer un atelier au deep learning. « Du coup, on a organisé un workshop pirate sur une demi-journée, en affrétant des bus pour emmener les conférenciers », raconte le « conspirateur » Environ 300 personnes, soit la moitié des participants, y assistent. « Cela a fait prendre conscience à la communauté qu'il se passait quelque chose d'intéressant. »
GPU = véhicule de la nouvelle révolution du deep learning: expérimentation sur GPU pour faire tourner les algo de DL
deep learning conspiracy : 2004 programme sur 5 ns du CIFAR (Canadian Institute for Advanced Research) nommé NCAP (Neural Computation and Adaptive Perception, Hinton directeur, Le Cun conseiller scientifique

## 10’s: Analyses textuelles, Graph Mining, Deep Learning

2011 - Watson IBM
2012 :  AlexNet (dataset: imagenet ILSVRC = 1,2 x 10^6 examples et 1000 classes / compute : 2 NVIDIA GTX 580 = 3162 GFLOPS / \approx 60x10^6 parameters, 8 layers, ReLU activation function = https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
2016 - AlphaGO - Lee Sedol 1à ans / AlphaGo Move 37 contre Lee seedol : we thought it was a mistake but actually great move!
2017- Transformer - Attention is all you need
2011: IBM Watson beats champions Ken Jennings and Brad Rutter at Jeopardy!
2012 : AlexNet paper : ImageNet Classification with Deep Convolutional Neural Networks : https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html
2015: Baidu’s Minwa supercomputer uses a special kind of deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.
2015 – Over 3,000 AI and Robotics researchers, endorsed by Stephen Hawking, Elon Musk and Steve Wozniak (among many others), sign an open letter warning of the danger of autonomous weapons which select and engage targets without human intervention.
2016: DeepMind’s AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Later, Google purchased DeepMind for a reported USD 400 million.

## 20

2020: GPT-3 (dataset: common crawl, webtetxt, wikipedia, other \approx 500 billion training tokens et \approx 100k unique tokens / compute: 10 000 NVIDIA V100 GPUS = 1 ExaFLOPS / 175B parameters on 96 layers, transformer)
2023: A rise in [[large-language-model]], or LLMs, such as ChatGPT, create an enormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pre-trained on vast amounts of raw, unlabeled data.
2023: GPT-4 (dataset: \approx 13T tokens / 25 000 NVIDIA A100s = \approx 4 ExaFLOPS / \approx 1T parameters on 120 layers, transformer)

## GLOPS:

- cerveau humain $86 \times 10^9$ neuronnes interconnectés par $1,5 \times 10^14$ synapses. Chacune de ces synapses peut effectuer $\approx 100$ calculs/secondes soit $1,5 \times 10^18$ FLOPS pour une consomation de 25 watts


Artificial Intelligence is a field of computer Science and encompasses [[machine-learning]] and [[deep-learning]]. It has gone through many cycles. Artificial General Intelligence is a long way from here we stand, despite some [parks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712).

The excitement surrounding foundation models like GPT and Llama has led to a flood of research papers—some insightful, some less so, and a few worthy of a place in the AI hall of fame. Understanding the history behind these developments is crucial, as it shows us how we got to where we are today. By grasping the fundamental building blocks of AI, you'll find it much easier to navigate the latest trends. [reisHistoryMatters2024]

Let’s embark on a journey through the fundamental milestones of artificial intelligence

Innovation does not appear from the blue. We are staying on the shoulder of giants. One day these giants will be Yan Le Cun, Yoshua Bnegio, Geooffrey Hinton, for future deep learning engineer yet to become. Like Alan Turing is our today giant...

## 60's: Perceptron and first neural network
## 70's and 80's: Régressions pénalisées et arbres
## 90's: Support Vector Machine et Random Forests
## 00's: Méthodes d'agrégation, Boosting
## 10's: Analyses textuelles, Graph Mining, Deep Learning

## Good Old Fashion Artificial Intelligence

In the 70' and 80', artificial intelligence uses symbolic and logic reasoning, which has been called _expert system_ and were called Good Old Fashion Artificial Intelligence, GOFAI, by the research community. [@LecunnLamachineapprend2023]

In this essay, let’s deep dive into paper that built the giant.

1. Perceptron 1957
2. CNN Le Cun
3. AlexNet paper : [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
3. AlphaGo Move 37 contre Lee seedol : we thought it was a mistake but actually great move!
4. Attention is all you need

title: "History of Artificial Intelligence"
---

The idea of "a machine that thinks" dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the
topics discussed in this article) important events and milestones in the evolution of [[artificial-intelligence]] include the following:

- **1950**: Alan Turing publishes [Computing Machinery and Intelligence](https://courses.cs.umbc.edu/471/papers/turing.pdf). In this paper, Turing—famous for breaking the German ENIGMA code during WWII and often referred to as the "father of computer science"— asks the following question: "Can machines think?"  From there, he offers a test, now famously known as the "Turing Test," where a human interrogator would try to distinguish between a computer and human text response. To pass the test, a computer must be able to fool a human into believing it is also human. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, as well as an ongoing concept within philosophy as it utilizes ideas around linguistics.

- **1952** — Arthur Samuel wrote the first computer learning program. The program was the game of checkers, and the IBM computer improved at the game the more it played, studying which moves made up winning strategies and incorporating those moves into its program.

- **1956**: John McCarthy coins the term "artificial intelligence" at the first-ever AI conference at Dartmouth College. (McCarthy would go on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program.

- **1967** (or 1957??): Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that "learned" though trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research projects.

- 1967 — The “nearest neighbor” algorithm was written, allowing computers to begin using very basic pattern recognition. This could be used to map a route for traveling salesmen, starting at a random city but ensuring they visit all cities during a short tour.

- 1979 — Students at Stanford University invent the “Stanford Cart” which can navigate obstacles in a room on its own.

- **1980s**: Neural networks which use a backpropagation algorithm to train itself become widely used in AI applications.

- 1981 — Gerald Dejong introduces the concept of Explanation Based Learning (EBL), in which a computer analyses training data and creates a general rule it can follow by discarding unimportant data.

- 1985 — Terry Sejnowski invents NetTalk, which learns to pronounce words the same way a baby does.

- 1990s — Work on machine learning shifts from a knowledge-driven approach to a data-driven approach.  Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions — or “learn” — from the results.

- **1995**: Stuart Russell and Peter Norvig publish [Artificial Intelligence: A Modern Approach](https://aima.cs.berkeley.edu/), which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems on the basis of rationality and thinking vs. acting.

- **1997**: IBM's Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).

- **2004**: John McCarthy writes a paper, [What Is Artificial Intelligence?](https://www-formal.stanford.edu/jmc/whatisai.pdf), and proposes an often-cited definition of AI.

- 2006 — Geoffrey Hinton coins the term “deep learning” to explain new algorithms that let computers “see” and distinguish objects and text in images and videos.

- **2011**: IBM Watson beats champions Ken Jennings and Brad Rutter at Jeopardy!

- **2015**: Baidu's Minwa supercomputer uses a special kind of deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.

- 2015 – Over 3,000 AI and Robotics researchers, endorsed by Stephen Hawking, Elon Musk and Steve Wozniak (among many others), sign an open letter warning of the danger of autonomous weapons which select and engage targets without human intervention.

- **2016**: DeepMind's AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Later, Google purchased DeepMind for a reported USD 400 million.

- **2023**: A rise in [[large-language-model]], or LLMs, such as ChatGPT, create an enormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pre-trained on vast amounts of raw, unlabeled data.
