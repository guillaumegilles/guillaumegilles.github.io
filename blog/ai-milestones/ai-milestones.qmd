---
title: AI Milestones
---

Artificial intelligence, or AI, is technology that enables computers and machines to simulate human intelligence and problem-solving capabilities.

Artificial Intelligence is a field of computer Science and encompasses [[machine-learning]] and [[deep-learning]]. It has gone through many cycles. Artificial General Intelligence is a long way from here we stand, despite some parks of Artificial General Intelligence: Early experiments with GPT-4.

The excitement surrounding foundation models like GPT and Llama has led to a flood of research papers—some insightful, some less so, and a few worthy of a place in the AI hall of fame. Understanding the history behind these developments is crucial, as it shows us how we got to where we are today. By grasping the fundamental building blocks of AI, you’ll find it much easier to navigate the latest trends. [reisHistoryMatters2024]

Let’s embark on a journey through the fundamental milestones of artificial intelligence

Innovation does not appear from the blue. We are staying on the shoulder of giants. One day these giants will be Yan Le Cun, Yoshua Bnegio, Geooffrey Hinton, for future deep learning engineer yet to become. Like Alan Turing is our today giant…

60’s: Perceptron and first neural network
70’s and 80’s: Régressions pénalisées et arbres
90’s: Support Vector Machine et Random Forests
00’s: Méthodes d’agrégation, Boosting
10’s: Analyses textuelles, Graph Mining, Deep Learning
Good Old Fashion Artificial Intelligence
In the 70’ and 80’, artificial intelligence uses symbolic and logic reasoning, which has been called expert system and were called Good Old Fashion Artificial Intelligence, GOFAI, by the research community. [@LecunnLamachineapprend2023]

In this essay, let’s deep dive into paper that built the giant.

Perceptron 1957
CNN Le Cun
AlexNet paper : ImageNet Classification with Deep Convolutional Neural Networks
AlphaGo Move 37 contre Lee seedol : we thought it was a mistake but actually great move!
Attention is all you need
title: “History of Artificial Intelligence”

The idea of “a machine that thinks” dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the topics discussed in this article) important events and milestones in the evolution of [[artificial-intelligence]] include the following:

1950: Alan Turing publishes Computing Machinery and Intelligence. In this paper, Turing—famous for breaking the German ENIGMA code during WWII and often referred to as the “father of computer science”— asks the following question: “Can machines think?” From there, he offers a test, now famously known as the “Turing Test,” where a human interrogator would try to distinguish between a computer and human text response. To pass the test, a computer must be able to fool a human into believing it is also human. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, as well as an ongoing concept within philosophy as it utilizes ideas around linguistics.

1952 — Arthur Samuel wrote the first computer learning program. The program was the game of checkers, and the IBM computer improved at the game the more it played, studying which moves made up winning strategies and incorporating those moves into its program.

1956: John McCarthy coins the term “artificial intelligence” at the first-ever AI conference at Dartmouth College. (McCarthy would go on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program.

1967 (or 1957??): Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that “learned” though trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research projects.

1967 — The “nearest neighbor” algorithm was written, allowing computers to begin using very basic pattern recognition. This could be used to map a route for traveling salesmen, starting at a random city but ensuring they visit all cities during a short tour.

1979 — Students at Stanford University invent the “Stanford Cart” which can navigate obstacles in a room on its own.

1980s: Neural networks which use a backpropagation algorithm to train itself become widely used in AI applications.

1981 — Gerald Dejong introduces the concept of Explanation Based Learning (EBL), in which a computer analyses training data and creates a general rule it can follow by discarding unimportant data.

1985 — Terry Sejnowski invents NetTalk, which learns to pronounce words the same way a baby does.

1990s — Work on machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions — or “learn” — from the results.

1995: Stuart Russell and Peter Norvig publish Artificial Intelligence: A Modern Approach, which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems on the basis of rationality and thinking vs. acting.

1997: IBM’s Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).

2004: John McCarthy writes a paper, What Is Artificial Intelligence?, and proposes an often-cited definition of AI.

2006 — Geoffrey Hinton coins the term “deep learning” to explain new algorithms that let computers “see” and distinguish objects and text in images and videos.

2011: IBM Watson beats champions Ken Jennings and Brad Rutter at Jeopardy!

2015: Baidu’s Minwa supercomputer uses a special kind of deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.

2015 – Over 3,000 AI and Robotics researchers, endorsed by Stephen Hawking, Elon Musk and Steve Wozniak (among many others), sign an open letter warning of the danger of autonomous weapons which select and engage targets without human intervention.

2016: DeepMind’s AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Later, Google purchased DeepMind for a reported USD 400 million.

2023: A rise in [[large-language-model]], or LLMs, such as ChatGPT, create an enormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pre-trained on vast amounts of raw, unlabeled data.
