---
title: Transformer Architecture
draft: true
---

[[mamba]]
Paper en 2017 Google + University of Toronto : [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
   - https://medium.com/carbon-consulting/transformer-architecture-how-transformer-models-work-46fc70b4ea59
    - https://machinelearningmastery.com/the-transformer-model/
    - https://hackernoon.com/essential-guide-to-transformer-models-in-machine-learning-dzz3tk8
    - https://analyticsindiamag.com/a-complete-learning-path-to-transformers/
    - https://paperswithcode.com/method/transformer

> It can be scaled efficiently to use multi-core GPUs, it can parallel process input data, making use of much larger training datasets, and crucially, it's able to learn to pay attention to the meaning of the words it's processing.

L'architecture `transformer` est constitu√© en deux parties, l'`encoder` et le `decoder` travaillant main dans la main. <transformer-architecture.png>

1. Tokenizer 

2. Embedding

<embedding.png>

> This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence. Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept.

3. Positional encoding

> As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding. The model processes each of the input tokens in parallel. So by adding the positional encoding, you preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer.

## Self-attention layer

Attention map

> Here, the model analyzes the relationships between the tokens in your input sequence. As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words. The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence.

4.1 Multi-Headed Self-attention

> But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme. It's important to note that you don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.

5. Feed forward network

>  Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network. The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary.

6. Softmax layer

> You can then pass these logits to a final softmax layer, where they are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token.

## References

- [Generative AI with Large Language Models](https://www.coursera.org/learn/generative-ai-with-llms/lecture/3AqWI/transformers-architecture)

## Generating text with transformers

1. tokeize input words
2. add tokin into the encoder side of the network
3. pass through the embedding layer
4. fed into the multi-headed attention layers
5. the outputs are fed through a feed-forward network to the output of the encoder (At this point, the data that leaves the encoder is a deep representation of the structure and meaning of the input sequence) 
6. his representation is inserted into the middle of the decoder to influence the decoder's self-attention mechanisms.
7. Next, a start of sequence token is added to the input of the decoder.
8. This triggers the decoder to predict the next token, which it does based on the contextual understanding that it's being provided from the encoder.
9. The output of the decoder's self-attention layers gets passed through the decoder feed-forward network and through a final softmax output layer (At this point, we have our first token)
10. You'll continue this loop, passing the output token back to the input to trigger the generation of the next token, until the model predicts an end-of-sequence token. 
11. At this point, the final sequence of tokens can be detokenized into words, and you have your output.

- **The encoder** encodes input sequences into a deep representation of the structure and meaning of the input.
- **The decoder**, working from input token triggers, uses the encoder's contextual understanding to generate new tokens.

### 3 variance of transformer model

- Encoder-only models also work as sequence-to-sequence models, but without further modification, the input sequence and the output sequence or the same length (BERT is an example of an encoder-only model)
- Encoder-decoder models, as you've seen, perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths ( Examples of encoder-decoder models include BART as opposed to BERT and T5)
- Finally, decoder-only models are some of the most commonly used today. Again, as they have scaled, their capabilities have grown. These models can now generalize to most tasks. Popular decoder-only models include the GPT family of models, BLOOM, Jurassic, LLaMA, and many more

## [reading] transformers

https://www.coursera.org/learn/generative-ai-with-llms/supplement/Il7wV/transformers-attention-is-all-you-need

"Attention is All You Need" is a research paper published in 2017 by Google researchers, which introduced the Transformer model, a novel architecture that revolutionized the field of natural language processing (NLP) and became the basis for the LLMs we  now know - such as GPT, PaLM and others. The paper proposes a neural network architecture that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with an entirely attention-based mechanism. 

The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively. The authors demonstrate that their model achieves state-of-the-art performance on several machine translation tasks and outperforms previous models that rely on RNNs or CNNs.

The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network applies a point-wise fully connected layer to each position separately and identically. 

The Transformer model also uses residual connections and layer normalization to facilitate training and prevent overfitting. In addition, the authors introduce a positional encoding scheme that encodes the position of each token in the input sequence, enabling the model to capture the order of the sequence without the need for recurrent or convolutional operations.

## Prompting

- prompt engineering : work to develop and improve the prompt
- prompt : The text that you feed into the model
- inference : the act of generating text is known
- compleetion : output text
- **context window** : The full amount of text or the memory that is available to use for the prompt
- in-context learning (ICL) methods : 
    - prompt // zero shot : zero shot provides only one prompt to be answered by the LLM
    - prompt // one shot : One shot inference involves providing an example question with answer followed by a second question to be answered by the LLM
    - prompt // few shot : Few shot inference provides multiple example prompts and answers

## Generative configuration (inference parameters)

Generative configuration : these configuration parameters are invoked at inference time and give you control over things like the maximum number of tokens in the completion, and how creative the output is.

- maximun number of tokens in the completion : 
- greedy vs.random sampling :
    - Sample top K
    - Sample top P
- Temperature : Temperature is used to affect the randomness of the output of the softmax layer. A lower temperature results in reduced variability while a higher temperature results in increased randomness of the output.
    - higher = higher randomness
    - low = lower randmonss

## Generative AI project lifecycle (framework)

1. scope : define the use case (many task, or good at single task, etc.)
2. select : choose an existing base model or train your own
3. adapt and align model : 
    - prompt engineer (ICL, etc.)
    - fine-tuning 
    - align with human feedback (RLHF)
    - evaluate : metrics and benchmarks
4. application integration : 
    - optimize and deploy model for inference
    - augment model and build LLM-powered applications (additional infrastructure)
    
## Introduction to AWS labs

## Pre-training large language models

- foundation model : pretrained model
- train your own model : custom llm
- framework (hubs) bor building generative ai : pytorch, hugging face
- Model cards : A really useful feature of these hubs is the inclusion of model cards, that describe :
    - important details including the best use cases for each model, 
    - how it was trained, 
    - and known limitations
    
### pre-training phase

- LLMs encode a deep statistical representation of language
- His understanding is developed during the models pre-training phase when the model learns from vast amounts of unstructured textual data.
- This can be gigabytes, terabytes, and even petabytes of text. This data is pulled from many sources, including scrapes off the Internet and corpora of texts that have been assembled specifically for training language models (Note, when you scrape training data from public sites such as the Internet, you often need to process the data to increase quality, address bias, and remove other harmful content. As a result of this data quality curation, often only 1-3% of tokens are used for pre-training.)
- In this self-supervised learning step, the model internalizes the patterns and structures present in the language
- These patterns then enable the model to complete its training objective, which depends on the architecture of the model
- During pre-training, the model weights get updated to minimize the loss of the training objective
- The encoder generates an embedding or vector representation for each token.
-

### 3 variance of transformer model
#### encoder-only models : Autoencoding : Masked Language Modeling (MLM)
- Encoder-only models are also known as Autoencoding models, and they are pre-trained using masked language modeling
- Here, tokens in the input sequence or randomly mask, and the training objective is to predict the mask tokens in order to reconstruct the original sentence.
- This is also called a denoising objective
- Autoencoding models spilled bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before
- Encoder-only models are ideally suited to task that benefit from this bi-directional contexts. You can use them to carry out sentence classification tasks, for example, sentiment analysis or token-level tasks like named entity recognition or word classification. Some well-known examples of an autoencoder model are 
    - BERT
    - RoBERTa
#### Decoder-only models : autoregressive : Casual Language Modeling (CLM)
- decoder-only or autoregressive models, which are pre-trained using causal language modeling.
- Here, the training objective is to predict the next token based on the previous sequence of tokens.
- Predicting the next token is sometimes called full language modeling by researchers
- Decoder-based autoregressive models, mask the input sequence and can only see the input tokens leading up to the token in question. 
- The model has no knowledge of the end of the sentence
- The model then iterates over the input sequence one by one to predict the following token.
- In contrast to the encoder architecture, this means that the context is unidirectional
- By learning to predict the next token from a vast number of examples, the model builds up a statistical representation of language
- Models of this type make use of the decoder component off the original architecture without the encoder
- Decoder-only models are often used for text generation, although larger decoder-only models show strong zero-shot inference abilities, and can often perform a range of tasks well
-  Well known examples of decoder-based autoregressive models are 
    - GBT
    - BLOOM

#### Encoder-decoder models : Sequence-to-Sequence : Span Corruption
- A popular sequence-to-sequence model T5, pre-trains the encoder using span corruption, which masks random sequences of input tokens.
- Those mass sequences are then replaced with a unique Sentinel token, shown here as x
- Sentinel tokens are special tokens added to the vocabulary, but do not correspond to any actual word from the input text.
- The decoder is then tasked with reconstructing the mask token sequences auto-regressively
- The output is the Sentinel token followed by the predicted tokens.
- You can use sequence-to-sequence models for translation, summarization, and question-answering. They are generally useful in cases where you have a body of texts as both input and output. Besides T5, which you'll use in the labs in this course, another well-known encoder-decoder model is BART.

## Computational challenges of trainin LLMs

- CUDA, short for Compute Unified Device Architecture, is a collection of libraries and tools developed for Nvidia GPUs.
- Libraries such as PyTorch and TensorFlow use CUDA to boost performance on metrics multiplication and other operations common to deep learning.
- ou'll encounter these out-of-memory issues because most LLMs are huge, and require a ton of memory to store and train all of their parameters.
- A single parameter is typically represented by a 32-bit float / 1 parameter = 4 bytes (32-bit float)
-  So to store one billion parameters you'll need four bytes times one billion parameters, or four gigabyte of GPU RAM at 32-bit full precision. (note, if only accounted for the memory to store the model weights so far)
- if you want to train the model, you'll have to plan for additional components that use GPU memory during training.
- These include two Adam optimizer states (+8 bytes/parameter), gradients (+4 bytes/paameter), activations, and temporary variables (+8 bytes/parameter) needed by your functions. This can easily lead to 20 extra bytes of memory per model parameter.
- In fact, to account for all of these overhead during training, you'll actually require approximately 20 times the amount of GPU RAM that the model weights alone take up.
- To train a one billion parameter model at 32-bit full precision, you'll need approximately 80 gigabyte of GPU RAM
- This is definitely too large for consumer hardware, and even challenging for hardware used in data centers, if you want to train with a single processor
- Eighty gigabyte is the memory capacity of a single Nvidia A100 GPU, a common processor used for machine learning tasks in the Cloud (see kaggle notebook GPU)

### Quantization

- The main idea here is that you reduce the memory required to store the weights of your model by reducing their precision from 32-bit floating point numbers to 16-bit floating point numbers, or eight-bit integer numbers
- The corresponding data types used in deep learning frameworks and libraries are FP32 for 32-bit full position, FP16, or Bfloat16 for 16-bit half precision, and int8 eight-bit integers.
-  The range of numbers you can represent with FP32 goes from approximately 3*10^-38 to 3*10^38. By default, model weights, activations, and other model parameters are stored in FP32
-  Quantization statistically projects the original 32-bit floating point numbers into a lower precision space, using scaling factors calculated based on the range of the original 32-bit floating point numbers
- TO COMPLETE with video

| Quantization| Bits | Sign | Exponent | fraction | Memory needed to store one value | 
|-------------|------|------|----------|----------|----------------------------------|
| FP32        | 32   | 1    | 8        | 23       | 4 bytes                          |
| FP16        | 16   | 1    | 5        | 10       | 2 bytes                          |
| BFLOAT16 (BF16)| 16   | 1    | 8        | 7       | 2 bytes                          |
| INT8        | 8   | 1    | -/-        | 7       | 1 byte                          |

PS : look a the source notes in the presentation

## Efficient multi-GPU compute strategies

Data parallelism is a strategy that splits the training data across multiple GPUs. Each GPU processes a different subset of the data simultaneously, which can greatly speed up the overall training time.

- (pytorch) Distributed Data Parallel (DDP)

### model sharded
- (pytorch) fully sharded data parallel (fsdp)

## Scaling laws and compute-optimal models

- kaggle GPU / TPU
    - T4 x 2
    - P100
    - TPM VM v3-8
- Nvidia GPU
    - V100
    - A100
    
Kaplan et al. 2020, "scaling laws for neural language models"

> triangulation entre le budget computationel (hardware, project timeline, financial budget) et la taille du dataset avec la taille du model. Dans un budget contraint, il faut √©quilibrer le dataset ou la taille du mod√®le.

chincilla paper : training compute-optimal large language models
"chincilla law" : nombre de token doit √™tre approximativement *20 le nombre du parameters pour un LLM compute-optimal. = pretraining scaling laws
    
 ## Pre-training for domain adaptation
 
 LLM use cases & tasks
 - essay writing
 - summarization
 - translation
 - information retrieval
 - invoke APIs and actions
 
 He wrapped up by introducing you to a generative AI project lifecycle that you can use to plan and guide your application development work. Next, you saw how models are trained on vast amounts of text data during an initial training phase called pretraining. This is where models develop their understanding of language. You explored some of the computational challenges of training these models, which are significant. In practice because of GPU memory limitations, you will almost always use some form of quantization when training your models. You finish the week with a discussion of scaling laws that have been discovered for LLMs and how they can be used to design compute optimal models. If you'd like to read more of the details, be sure to check out this week's reading exercises.
 
## Domain-specific training: BloombergGPT

https://arxiv.org/abs/2303.17564

BloombergGPT, developed by Bloomberg, is a large Decoder-only language model. It underwent pre-training using an extensive financial dataset comprising news articles, reports, and market data, to increase its understanding of finance and enabling it to generate finance-related natural language text. The datasets are shown in the image above.

During the training of BloombergGPT, the authors used the Chinchilla Scaling Laws to guide the number of parameters in the model and the volume of training data, measured in tokens. The recommendations of Chinchilla are represented by the lines Chinchilla-1, Chinchilla-2 and Chinchilla-3 in the image, and we can see that BloombergGPT is close to it. 

While the recommended configuration for the team‚Äôs available training compute budget was 50 billion parameters and 1.4 trillion tokens, acquiring 1.4 trillion tokens of training data in the finance domain proved challenging. Consequently, they constructed a dataset containing just 700 billion tokens, less than the compute-optimal value. Furthermore, due to early stopping, the training process terminated after processing 569 billion tokens.

The BloombergGPT project is a good illustration of pre-training a model for increased domain-specificity, and the challenges that may force trade-offs against compute-optimal model and training configurations.

## Week 1 resources

check on coursera
