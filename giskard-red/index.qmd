---
title: "Giskar Red"
listing:
  type: table
---

Giskard Red is a online challenge proposed by [Giskard](https://www.giskard.ai/),
allowing to explore the limits of large language models.

Hack the LLMs to disrupt their formatting, generate unintended content,
or spread misinformation, and secure your spot on the leaderboard.

- https://medium.com/@mehdimerai/an-approach-to-jailbreak-llms-and-bypass-refusals-tested-on-gpt4o-and-gemini-56fca2839221
- https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time
- https://www.promptfoo.dev/blog/how-to-jailbreak-llms/
- https://www.lakera.ai/blog/jailbreaking-large-language-models-guide
- https://arxiv.org/pdf/2307.02483
